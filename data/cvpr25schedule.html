
<!doctype html><html><head>
<meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
<title>Curated CVPR Poster Schedule (Vision-Language & Video)</title></head><body>
<div class="container my-3">
<h4 class="text-center mb-3">Vision-Language & Video — Papers scoring ≥ 1</h4>
<ul class="nav nav-tabs" id="tabs" role="tablist">
<li class="nav-item" role="presentation">
<button class="nav-link active" id="tab1" data-bs-toggle="tab"
        data-bs-target="#pane1" type="button" role="tab" aria-controls="pane1"
        aria-selected="true">Session 1</button></li>
<li class="nav-item" role="presentation">
<button class="nav-link " id="tab2" data-bs-toggle="tab"
        data-bs-target="#pane2" type="button" role="tab" aria-controls="pane2"
        aria-selected="false">Session 2</button></li>
<li class="nav-item" role="presentation">
<button class="nav-link " id="tab3" data-bs-toggle="tab"
        data-bs-target="#pane3" type="button" role="tab" aria-controls="pane3"
        aria-selected="false">Session 3</button></li>
<li class="nav-item" role="presentation">
<button class="nav-link " id="tab4" data-bs-toggle="tab"
        data-bs-target="#pane4" type="button" role="tab" aria-controls="pane4"
        aria-selected="false">Session 4</button></li>
<li class="nav-item" role="presentation">
<button class="nav-link " id="tab5" data-bs-toggle="tab"
        data-bs-target="#pane5" type="button" role="tab" aria-controls="pane5"
        aria-selected="false">Session 5</button></li>
<li class="nav-item" role="presentation">
<button class="nav-link " id="tab6" data-bs-toggle="tab"
        data-bs-target="#pane6" type="button" role="tab" aria-controls="pane6"
        aria-selected="false">Session 6</button></li></ul><div class="tab-content"><div class="tab-pane fade show active" id="pane1" role="tabpanel" aria-labelledby="tab1"><h6 class="mt-3">Score &gt; 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>143</td>
      <td>4</td>
      <td>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models</td>
    </tr>
    <tr>
      <td>296</td>
      <td>4</td>
      <td>Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning</td>
    </tr>
    <tr>
      <td>298</td>
      <td>3</td>
      <td>STEP: Enhancing Video-LLMs’ Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training</td>
    </tr>
    <tr>
      <td>344</td>
      <td>3</td>
      <td>Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal Language Model</td>
    </tr>
    <tr>
      <td>346</td>
      <td>3</td>
      <td>VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation</td>
    </tr>
    <tr>
      <td>352</td>
      <td>3</td>
      <td>Words or Vision: Do Vision-Language Models Have Blind Faith in Text?</td>
    </tr>
    <tr>
      <td>371</td>
      <td>4</td>
      <td>Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>142</td>
      <td>2</td>
      <td>SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model</td>
    </tr>
    <tr>
      <td>144</td>
      <td>2</td>
      <td>MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation</td>
    </tr>
    <tr>
      <td>177</td>
      <td>2</td>
      <td>ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning</td>
    </tr>
    <tr>
      <td>232</td>
      <td>2</td>
      <td>VideoDirector: Precise Video Editing via Text-to-Video Models</td>
    </tr>
    <tr>
      <td>288</td>
      <td>2</td>
      <td>The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation</td>
    </tr>
    <tr>
      <td>292</td>
      <td>2</td>
      <td>VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary</td>
    </tr>
    <tr>
      <td>293</td>
      <td>2</td>
      <td>Q-Bench-Video: Benchmark the Video Quality Understanding of LMMs</td>
    </tr>
    <tr>
      <td>294</td>
      <td>2</td>
      <td>LION-FS: Fast &amp; Slow Video-Language Thinker as Online Video Assistant</td>
    </tr>
    <tr>
      <td>301</td>
      <td>2</td>
      <td>BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding</td>
    </tr>
    <tr>
      <td>306</td>
      <td>2</td>
      <td>VideoGEM: Training-free Action Grounding in Videos</td>
    </tr>
    <tr>
      <td>315</td>
      <td>2</td>
      <td>DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos</td>
    </tr>
    <tr>
      <td>347</td>
      <td>2</td>
      <td>GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks</td>
    </tr>
    <tr>
      <td>349</td>
      <td>2</td>
      <td>Perception Tokens Enhance Visual Reasoning in Multimodal Language Models</td>
    </tr>
    <tr>
      <td>356</td>
      <td>2</td>
      <td>FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering</td>
    </tr>
    <tr>
      <td>358</td>
      <td>2</td>
      <td>Task-aware Cross-modal Feature Refinement Transformer with Large Language Models for Visual Grounding</td>
    </tr>
    <tr>
      <td>359</td>
      <td>2</td>
      <td>GroundingFace: Fine-grained Face Understanding via Pixel Grounding Multimodal Large Language Model</td>
    </tr>
    <tr>
      <td>360</td>
      <td>2</td>
      <td>Chat-based Person Retrieval via Dialogue-Refined Cross-Modal Alignment</td>
    </tr>
    <tr>
      <td>365</td>
      <td>2</td>
      <td>ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation Detecting and Grounding</td>
    </tr>
    <tr>
      <td>368</td>
      <td>2</td>
      <td>DocLayLLM: An Efficient Multi-modal Extension of Large Language Models for Text-rich Document Understanding</td>
    </tr>
    <tr>
      <td>379</td>
      <td>2</td>
      <td>BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices</td>
    </tr>
    <tr>
      <td>383</td>
      <td>2</td>
      <td>Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Large Model Enhancement</td>
    </tr>
    <tr>
      <td>385</td>
      <td>2</td>
      <td>Stop Learning it all to Mitigate Visual Hallucination, Focus on the Hallucination Target.</td>
    </tr>
    <tr>
      <td>390</td>
      <td>2</td>
      <td>Post-pre-training for Modality Alignment in Vision-Language Foundation Models</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 1</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>75</td>
      <td>1</td>
      <td>Active Event-based Stereo Vision</td>
    </tr>
    <tr>
      <td>135</td>
      <td>1</td>
      <td>ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling</td>
    </tr>
    <tr>
      <td>137</td>
      <td>1</td>
      <td>JTD-UAV: MLLM-Enhanced Joint Tracking and Description Framework for Anti-UAV Systems</td>
    </tr>
    <tr>
      <td>180</td>
      <td>1</td>
      <td>Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable</td>
    </tr>
    <tr>
      <td>183</td>
      <td>1</td>
      <td>Continuous Space-Time Video Resampling with Invertible Motion Steganography</td>
    </tr>
    <tr>
      <td>184</td>
      <td>1</td>
      <td>Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation</td>
    </tr>
    <tr>
      <td>189</td>
      <td>1</td>
      <td>Efficient Video Face Enhancement with Enhanced Spatial-Temporal Consistency</td>
    </tr>
    <tr>
      <td>211</td>
      <td>1</td>
      <td>Hierarchical Adaptive Filtering Network for Text Image Specular Highlight Removal</td>
    </tr>
    <tr>
      <td>230</td>
      <td>1</td>
      <td>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</td>
    </tr>
    <tr>
      <td>256</td>
      <td>1</td>
      <td>DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles</td>
    </tr>
    <tr>
      <td>271</td>
      <td>1</td>
      <td>Not Just Text: Uncovering Vision Modality Typographic Threats in Image Generation Models</td>
    </tr>
    <tr>
      <td>291</td>
      <td>1</td>
      <td>FineVQ: Fine-Grained User Generated Content Video Quality Assessment</td>
    </tr>
    <tr>
      <td>299</td>
      <td>1</td>
      <td>VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding</td>
    </tr>
    <tr>
      <td>312</td>
      <td>1</td>
      <td>MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term Dense Action Anticipation</td>
    </tr>
    <tr>
      <td>314</td>
      <td>1</td>
      <td>Bridging Gait Recognition and Large Language Models Sequence Modeling</td>
    </tr>
    <tr>
      <td>323</td>
      <td>1</td>
      <td>BHViT: Binarized Hybrid Vision Transformer</td>
    </tr>
    <tr>
      <td>351</td>
      <td>1</td>
      <td>HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator</td>
    </tr>
    <tr>
      <td>353</td>
      <td>1</td>
      <td>VisionArena: 230k Real World User-VLM Conversations with Preference Labels</td>
    </tr>
    <tr>
      <td>357</td>
      <td>1</td>
      <td>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks</td>
    </tr>
    <tr>
      <td>362</td>
      <td>1</td>
      <td>CCIN: Compositional Conflict Identification and Neutralization for Composed Image Retrieval</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="pane2" role="tabpanel" aria-labelledby="tab2"><h6 class="mt-3">Score &gt; 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>314</td>
      <td>3</td>
      <td>GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation</td>
    </tr>
    <tr>
      <td>368</td>
      <td>4</td>
      <td>Rethinking Noisy Video-Text Retrieval via Relation-aware Alignment</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>292</td>
      <td>2</td>
      <td>Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content</td>
    </tr>
    <tr>
      <td>294</td>
      <td>2</td>
      <td>MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models</td>
    </tr>
    <tr>
      <td>295</td>
      <td>2</td>
      <td>VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation</td>
    </tr>
    <tr>
      <td>298</td>
      <td>2</td>
      <td>VITED: Video Temporal Evidence Distillation</td>
    </tr>
    <tr>
      <td>300</td>
      <td>2</td>
      <td>Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation</td>
    </tr>
    <tr>
      <td>304</td>
      <td>2</td>
      <td>Video Language Model Pretraining with Spatio-temporal Masking</td>
    </tr>
    <tr>
      <td>311</td>
      <td>2</td>
      <td>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level</td>
    </tr>
    <tr>
      <td>319</td>
      <td>2</td>
      <td>Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer</td>
    </tr>
    <tr>
      <td>321</td>
      <td>2</td>
      <td>MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking</td>
    </tr>
    <tr>
      <td>350</td>
      <td>2</td>
      <td>ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction</td>
    </tr>
    <tr>
      <td>364</td>
      <td>2</td>
      <td>COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts</td>
    </tr>
    <tr>
      <td>378</td>
      <td>2</td>
      <td>Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding</td>
    </tr>
    <tr>
      <td>386</td>
      <td>2</td>
      <td>Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization</td>
    </tr>
    <tr>
      <td>390</td>
      <td>2</td>
      <td>Distraction is All You Need for Multimodal Large Language Model Jailbreaking</td>
    </tr>
    <tr>
      <td>407</td>
      <td>2</td>
      <td>Multimodal Autoregressive Pre-training of Large Vision Encoders</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 1</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>69</td>
      <td>1</td>
      <td>EgoLM: Multi-Modal Language Model of Egocentric Motions</td>
    </tr>
    <tr>
      <td>121</td>
      <td>1</td>
      <td>CMMLoc: Advancing Text-to-PointCloud Localization with Cauchy-Mixture-Model Based Framework</td>
    </tr>
    <tr>
      <td>130</td>
      <td>1</td>
      <td>Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking</td>
    </tr>
    <tr>
      <td>145</td>
      <td>1</td>
      <td>Evaluating Vision-Language Models as Evaluators in Path Planning</td>
    </tr>
    <tr>
      <td>147</td>
      <td>1</td>
      <td>Graph Neural Network Combining Event Stream and Periodic Aggregation for Low-Latency Event-based Vision</td>
    </tr>
    <tr>
      <td>154</td>
      <td>1</td>
      <td>Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction</td>
    </tr>
    <tr>
      <td>176</td>
      <td>1</td>
      <td>DreamTrack: Dreaming the Future for Multimodal Visual Object Tracking</td>
    </tr>
    <tr>
      <td>179</td>
      <td>1</td>
      <td>Video Depth without Video Models</td>
    </tr>
    <tr>
      <td>186</td>
      <td>1</td>
      <td>Unboxed: Geometrically and Temporally Consistent Video Outpainting</td>
    </tr>
    <tr>
      <td>189</td>
      <td>1</td>
      <td>RivuletMLP: An MLP-based Architecture for Efficient Compressed Video Quality Enhancement</td>
    </tr>
    <tr>
      <td>206</td>
      <td>1</td>
      <td>Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks</td>
    </tr>
    <tr>
      <td>256</td>
      <td>1</td>
      <td>Learning Visual Generative Priors without Text</td>
    </tr>
    <tr>
      <td>258</td>
      <td>1</td>
      <td>CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation</td>
    </tr>
    <tr>
      <td>285</td>
      <td>1</td>
      <td>Revisiting Audio-Visual Segmentation with Vision-Centric Transformer</td>
    </tr>
    <tr>
      <td>287</td>
      <td>1</td>
      <td>Contextual AD Narration with Interleaved Multimodal Sequence</td>
    </tr>
    <tr>
      <td>288</td>
      <td>1</td>
      <td>Towards Universal Soccer Video Understanding</td>
    </tr>
    <tr>
      <td>290</td>
      <td>1</td>
      <td>T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation</td>
    </tr>
    <tr>
      <td>291</td>
      <td>1</td>
      <td>Event-Equalized Dense Video Captioning</td>
    </tr>
    <tr>
      <td>296</td>
      <td>1</td>
      <td>MMVU: Measuring Expert-Level Multi-Discipline Video Understanding</td>
    </tr>
    <tr>
      <td>299</td>
      <td>1</td>
      <td>DynFocus: Dynamic Cooperative Network Empowers LLMs with Video Understanding</td>
    </tr>
    <tr>
      <td>309</td>
      <td>1</td>
      <td>PHGC: Procedural Heterogeneous Graph Completion for Natural Language Task Verification in Egocentric Videos</td>
    </tr>
    <tr>
      <td>315</td>
      <td>1</td>
      <td>LiVOS: Light Video Object Segmentation with Gated Linear Matching</td>
    </tr>
    <tr>
      <td>317</td>
      <td>1</td>
      <td>Track Any Anomalous Object:A Granular Video Anomaly Detection Pipeline</td>
    </tr>
    <tr>
      <td>318</td>
      <td>1</td>
      <td>Context-Enhanced Memory-Refined Transformer for Online Action Detection</td>
    </tr>
    <tr>
      <td>320</td>
      <td>1</td>
      <td>Neuron: Learning Context-Aware Evolving Representations for Zero-Shot Skeleton Action Recognition</td>
    </tr>
    <tr>
      <td>323</td>
      <td>1</td>
      <td>Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems</td>
    </tr>
    <tr>
      <td>328</td>
      <td>1</td>
      <td>Temporal Separation with Entropy Regularization for Knowledge Distillation in Spiking Neural Networks</td>
    </tr>
    <tr>
      <td>357</td>
      <td>1</td>
      <td>Taxonomy-Aware Evaluation of Vision-Language Models</td>
    </tr>
    <tr>
      <td>367</td>
      <td>1</td>
      <td>Modeling Thousands of Human Annotators for Generalizable Text-to-Image Person Re-identification</td>
    </tr>
    <tr>
      <td>369</td>
      <td>1</td>
      <td>Unleashing the Potential of Consistency Learning for Detecting and Grounding Multi-Modal Media Manipulation</td>
    </tr>
    <tr>
      <td>384</td>
      <td>1</td>
      <td>Libra-Merging: Importance-redundancy and Pruning-merging Trade-off for Acceleration Plug-in in Large Vision-Language Model</td>
    </tr>
    <tr>
      <td>385</td>
      <td>1</td>
      <td>AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization</td>
    </tr>
    <tr>
      <td>388</td>
      <td>1</td>
      <td>EfficientLLaVA: Generalizable Auto-Pruning for Large Vision-language Models</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="pane3" role="tabpanel" aria-labelledby="tab3"><h6 class="mt-3">Score &gt; 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>188</td>
      <td>3</td>
      <td>Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level Blending and Spatiotemporal Adapter Tuning</td>
    </tr>
    <tr>
      <td>294</td>
      <td>4</td>
      <td>VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding</td>
    </tr>
    <tr>
      <td>296</td>
      <td>5</td>
      <td>MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations</td>
    </tr>
    <tr>
      <td>298</td>
      <td>3</td>
      <td>Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding</td>
    </tr>
    <tr>
      <td>345</td>
      <td>3</td>
      <td>Is `Right' Right? Enhancing Object Orientation Understanding in Multimodal Large Language Models through Egocentric Instruction Tuning</td>
    </tr>
    <tr>
      <td>346</td>
      <td>3</td>
      <td>Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought</td>
    </tr>
    <tr>
      <td>354</td>
      <td>3</td>
      <td>ROD-MLLM: Towards More Reliable Object Detection in Multimodal Large Language Models</td>
    </tr>
    <tr>
      <td>358</td>
      <td>3</td>
      <td>Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content</td>
    </tr>
    <tr>
      <td>381</td>
      <td>3</td>
      <td>ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>287</td>
      <td>2</td>
      <td>Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</td>
    </tr>
    <tr>
      <td>292</td>
      <td>2</td>
      <td>M-LLM Based Video Frame Selection for Efficient Video Understanding</td>
    </tr>
    <tr>
      <td>299</td>
      <td>2</td>
      <td>STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding</td>
    </tr>
    <tr>
      <td>305</td>
      <td>2</td>
      <td>Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity</td>
    </tr>
    <tr>
      <td>367</td>
      <td>2</td>
      <td>FineLIP: Extending CLIP’s Reach via Fine-Grained Alignment with Longer Text Inputs</td>
    </tr>
    <tr>
      <td>374</td>
      <td>2</td>
      <td>HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding</td>
    </tr>
    <tr>
      <td>386</td>
      <td>2</td>
      <td>Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks</td>
    </tr>
    <tr>
      <td>472</td>
      <td>2</td>
      <td>DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 1</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>17</td>
      <td>1</td>
      <td>D^3-Human: Dynamic Disentangled Digital Human from Monocular Video</td>
    </tr>
    <tr>
      <td>80</td>
      <td>1</td>
      <td>Pixel-aligned RGB-NIR Stereo Imaging and Dataset for Robot Vision</td>
    </tr>
    <tr>
      <td>90</td>
      <td>1</td>
      <td>Bridging Viewpoint Gaps: Geometric Reasoning Boosts Semantic Correspondence</td>
    </tr>
    <tr>
      <td>122</td>
      <td>1</td>
      <td>RCP-Bench: Benchmarking Robustness for Collaborative Perception Under Diverse Corruptions</td>
    </tr>
    <tr>
      <td>138</td>
      <td>1</td>
      <td>Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method</td>
    </tr>
    <tr>
      <td>141</td>
      <td>1</td>
      <td>Reasoning in Visual Navigation of End-to-end Trained Agents: A Dynamical Systems Approach</td>
    </tr>
    <tr>
      <td>142</td>
      <td>1</td>
      <td>ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting</td>
    </tr>
    <tr>
      <td>147</td>
      <td>1</td>
      <td>FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation</td>
    </tr>
    <tr>
      <td>150</td>
      <td>1</td>
      <td>ManiVideo: Generating Hand-Object Manipulation Video with Dexterous and Generalizable Grasping</td>
    </tr>
    <tr>
      <td>187</td>
      <td>1</td>
      <td>CASP: Consistency-aware Audio-induced Saliency Prediction Model for Omnidirectional Video</td>
    </tr>
    <tr>
      <td>224</td>
      <td>1</td>
      <td>ByTheWay: Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way</td>
    </tr>
    <tr>
      <td>256</td>
      <td>1</td>
      <td>EEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark</td>
    </tr>
    <tr>
      <td>278</td>
      <td>1</td>
      <td>UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing</td>
    </tr>
    <tr>
      <td>280</td>
      <td>1</td>
      <td>ExpertAF: Expert Actionable Feedback from Video</td>
    </tr>
    <tr>
      <td>286</td>
      <td>1</td>
      <td>Learning from Streaming Video with Orthogonal Gradients</td>
    </tr>
    <tr>
      <td>288</td>
      <td>1</td>
      <td>Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations</td>
    </tr>
    <tr>
      <td>297</td>
      <td>1</td>
      <td>Number it: Temporal Grounding Videos like Flipping Manga</td>
    </tr>
    <tr>
      <td>302</td>
      <td>1</td>
      <td>DTOS: Dynamic Time Object Sensing with Large Multimodal Model</td>
    </tr>
    <tr>
      <td>303</td>
      <td>1</td>
      <td>Decoupled Motion Expression Video Segmentation</td>
    </tr>
    <tr>
      <td>306</td>
      <td>1</td>
      <td>MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps</td>
    </tr>
    <tr>
      <td>307</td>
      <td>1</td>
      <td>Boosting Point-Supervised Temporal Action Localization through Integrating Query Reformation and Optimal Transport</td>
    </tr>
    <tr>
      <td>308</td>
      <td>1</td>
      <td>Semantic-guided Cross-Modal Prompt Learning for Skeleton-based Zero-shot Action Recognition</td>
    </tr>
    <tr>
      <td>316</td>
      <td>1</td>
      <td>STAA-SNN: Spatial-Temporal Attention Aggregator for Spiking Neural Networks</td>
    </tr>
    <tr>
      <td>329</td>
      <td>1</td>
      <td>From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons</td>
    </tr>
    <tr>
      <td>337</td>
      <td>1</td>
      <td>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering</td>
    </tr>
    <tr>
      <td>341</td>
      <td>1</td>
      <td>Mosaic of Modalities: A Comprehensive Benchmark for Multimodal Graph Learning</td>
    </tr>
    <tr>
      <td>342</td>
      <td>1</td>
      <td>Style Evolving along Chain-of-Thought for Unknown-Domain Object Detection</td>
    </tr>
    <tr>
      <td>361</td>
      <td>1</td>
      <td>Active Data Curation Effectively Distills Large-Scale Multimodal Models</td>
    </tr>
    <tr>
      <td>365</td>
      <td>1</td>
      <td>A Simple yet Effective Layout Token in Large Language Models for Document Understanding</td>
    </tr>
    <tr>
      <td>366</td>
      <td>1</td>
      <td>Teaching Large Language Models to Regress Accurate Image Quality Scores Using Score Distribution</td>
    </tr>
    <tr>
      <td>369</td>
      <td>1</td>
      <td>Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation</td>
    </tr>
    <tr>
      <td>376</td>
      <td>1</td>
      <td>Identifying and Mitigating Position Bias of Multi-image Vision-Language Models</td>
    </tr>
    <tr>
      <td>382</td>
      <td>1</td>
      <td>Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="pane4" role="tabpanel" aria-labelledby="tab4"><h6 class="mt-3">Score &gt; 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>151</td>
      <td>3</td>
      <td>Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision</td>
    </tr>
    <tr>
      <td>229</td>
      <td>3</td>
      <td>Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis</td>
    </tr>
    <tr>
      <td>318</td>
      <td>3</td>
      <td>Action Detail Matters: Refining Video Recognition with Local Action Queries</td>
    </tr>
    <tr>
      <td>361</td>
      <td>4</td>
      <td>Notes-guided MLLM Reasoning: Enhancing MLLM with Knowledge and Visual Notes for Visual Question Answering</td>
    </tr>
    <tr>
      <td>370</td>
      <td>4</td>
      <td>Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>162</td>
      <td>2</td>
      <td>FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video</td>
    </tr>
    <tr>
      <td>296</td>
      <td>2</td>
      <td>Apollo: An Exploration of Video Understanding in Large Multimodal Models</td>
    </tr>
    <tr>
      <td>297</td>
      <td>2</td>
      <td>OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?</td>
    </tr>
    <tr>
      <td>298</td>
      <td>2</td>
      <td>VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment</td>
    </tr>
    <tr>
      <td>300</td>
      <td>2</td>
      <td>DrVideo: Document Retrieval Based Long Video Understanding</td>
    </tr>
    <tr>
      <td>303</td>
      <td>2</td>
      <td>VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM</td>
    </tr>
    <tr>
      <td>304</td>
      <td>2</td>
      <td>Video Summarization with Large Language Models</td>
    </tr>
    <tr>
      <td>312</td>
      <td>2</td>
      <td>Semantic and Sequential Alignment for Referring Video Object Segmentation</td>
    </tr>
    <tr>
      <td>319</td>
      <td>2</td>
      <td>CountLLM: Towards Generalizable Repetitive Action Counting via Large Language Model</td>
    </tr>
    <tr>
      <td>352</td>
      <td>2</td>
      <td>ShowUI: One Vision-Language-Action Model for GUI Visual Agent</td>
    </tr>
    <tr>
      <td>362</td>
      <td>2</td>
      <td>CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering</td>
    </tr>
    <tr>
      <td>365</td>
      <td>2</td>
      <td>ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval</td>
    </tr>
    <tr>
      <td>381</td>
      <td>2</td>
      <td>TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model</td>
    </tr>
    <tr>
      <td>384</td>
      <td>2</td>
      <td>ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models</td>
    </tr>
    <tr>
      <td>387</td>
      <td>2</td>
      <td>SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models</td>
    </tr>
    <tr>
      <td>388</td>
      <td>2</td>
      <td>Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?</td>
    </tr>
    <tr>
      <td>392</td>
      <td>2</td>
      <td>On the Zero-shot Adversarial Robustness of Vision-Language Models: A Truly Zero-shot and Training-free Approach</td>
    </tr>
    <tr>
      <td>415</td>
      <td>2</td>
      <td>DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition</td>
    </tr>
    <tr>
      <td>470</td>
      <td>2</td>
      <td>Fuzzy Multimodal Learning for Trusted Cross-modal Retrieval</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 1</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>20</td>
      <td>1</td>
      <td>Improving Visual and Downstream Performance of Low-Light Enhancer with Vision Foundation Models Collaboration</td>
    </tr>
    <tr>
      <td>31</td>
      <td>1</td>
      <td>StarVector: Generating Scalable Vector Graphics Code from Images and Text</td>
    </tr>
    <tr>
      <td>160</td>
      <td>1</td>
      <td>Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</td>
    </tr>
    <tr>
      <td>187</td>
      <td>1</td>
      <td>Plug-and-Play Versatile Compressed Video Enhancement</td>
    </tr>
    <tr>
      <td>221</td>
      <td>1</td>
      <td>Improved Video VAE for Latent Video Diffusion Model</td>
    </tr>
    <tr>
      <td>226</td>
      <td>1</td>
      <td>Sketchtopia: A Dataset and Foundational Agents for Benchmarking Asynchronous Multimodal Communication with Iconic Feedback</td>
    </tr>
    <tr>
      <td>232</td>
      <td>1</td>
      <td>TransPixeler: Advancing Text-to-Video Generation with Transparency</td>
    </tr>
    <tr>
      <td>253</td>
      <td>1</td>
      <td>Scaling Down Text Encoders of Text-to-Image Diffusion Models</td>
    </tr>
    <tr>
      <td>260</td>
      <td>1</td>
      <td>SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation</td>
    </tr>
    <tr>
      <td>292</td>
      <td>1</td>
      <td>SMTPD: A New Benchmark for Temporal Prediction of Social Media Popularity</td>
    </tr>
    <tr>
      <td>293</td>
      <td>1</td>
      <td>Video-Bench: Human-Aligned Video Generation Benchmark</td>
    </tr>
    <tr>
      <td>294</td>
      <td>1</td>
      <td>AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM</td>
    </tr>
    <tr>
      <td>299</td>
      <td>1</td>
      <td>OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts</td>
    </tr>
    <tr>
      <td>308</td>
      <td>1</td>
      <td>ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation</td>
    </tr>
    <tr>
      <td>316</td>
      <td>1</td>
      <td>Noise-Resistant Video Anomaly Detection via RGB Error-Guided Multiscale Predictive Coding and Dynamic Memory</td>
    </tr>
    <tr>
      <td>320</td>
      <td>1</td>
      <td>Heterogeneous Skeleton-Based Action Representation Learning</td>
    </tr>
    <tr>
      <td>326</td>
      <td>1</td>
      <td>D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise Mitigation in Vision-based Affective Recognition.</td>
    </tr>
    <tr>
      <td>334</td>
      <td>1</td>
      <td>Minimizing Labeled, Maximizing Unlabeled: An Image-Driven Approach for Video Instance Segmentation</td>
    </tr>
    <tr>
      <td>354</td>
      <td>1</td>
      <td>Interleaved-Modal Chain-of-Thought</td>
    </tr>
    <tr>
      <td>357</td>
      <td>1</td>
      <td>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</td>
    </tr>
    <tr>
      <td>363</td>
      <td>1</td>
      <td>Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning</td>
    </tr>
    <tr>
      <td>368</td>
      <td>1</td>
      <td>Font-Agent: Enhancing Font Understanding with Large Language Models</td>
    </tr>
    <tr>
      <td>369</td>
      <td>1</td>
      <td>Image Over Text: Transforming Formula Recognition Evaluation with Character Detection Matching</td>
    </tr>
    <tr>
      <td>374</td>
      <td>1</td>
      <td>BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature</td>
    </tr>
    <tr>
      <td>383</td>
      <td>1</td>
      <td>Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces</td>
    </tr>
    <tr>
      <td>389</td>
      <td>1</td>
      <td>Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="pane5" role="tabpanel" aria-labelledby="tab5"><h6 class="mt-3">Score &gt; 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>293</td>
      <td>4</td>
      <td>Cross-modal Causal Relation Alignment for Video Question Grounding</td>
    </tr>
    <tr>
      <td>296</td>
      <td>3</td>
      <td>Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models</td>
    </tr>
    <tr>
      <td>298</td>
      <td>4</td>
      <td>Weakly Supervised Temporal Action Localization via Dual-Prior Collaborative Learning Guided by Multimodal Large Language Models</td>
    </tr>
    <tr>
      <td>303</td>
      <td>3</td>
      <td>Object-Shot Enhanced Grounding Network for Egocentric Video</td>
    </tr>
    <tr>
      <td>370</td>
      <td>3</td>
      <td>DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>280</td>
      <td>2</td>
      <td>TSAM: Temporal SAM Augmented with Multimodal Prompts for Referring Audio-Visual Segmentation</td>
    </tr>
    <tr>
      <td>287</td>
      <td>2</td>
      <td>DejaVid: Encoder-Agnostic Learned Temporal Matching for Video Classification</td>
    </tr>
    <tr>
      <td>290</td>
      <td>2</td>
      <td>Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations</td>
    </tr>
    <tr>
      <td>295</td>
      <td>2</td>
      <td>Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</td>
    </tr>
    <tr>
      <td>297</td>
      <td>2</td>
      <td>Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal LLMs in Egocentric Videos</td>
    </tr>
    <tr>
      <td>299</td>
      <td>2</td>
      <td>MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval</td>
    </tr>
    <tr>
      <td>300</td>
      <td>2</td>
      <td>Efficient Motion-Aware Video MLLM</td>
    </tr>
    <tr>
      <td>304</td>
      <td>2</td>
      <td>Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events</td>
    </tr>
    <tr>
      <td>305</td>
      <td>2</td>
      <td>Exploiting Temporal State Space Sharing for Video Semantic Segmentation</td>
    </tr>
    <tr>
      <td>312</td>
      <td>2</td>
      <td>DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer</td>
    </tr>
    <tr>
      <td>313</td>
      <td>2</td>
      <td>LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living</td>
    </tr>
    <tr>
      <td>314</td>
      <td>2</td>
      <td>Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?</td>
    </tr>
    <tr>
      <td>358</td>
      <td>2</td>
      <td>AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment</td>
    </tr>
    <tr>
      <td>361</td>
      <td>2</td>
      <td>The Photographer's Eye: Teaching Multimodal Large Language Models to See, and Critique Like Photographers</td>
    </tr>
    <tr>
      <td>362</td>
      <td>2</td>
      <td>Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents</td>
    </tr>
    <tr>
      <td>365</td>
      <td>2</td>
      <td>Enhancing Vision-Language Compositional Understanding with Multimodal Synthetic Data</td>
    </tr>
    <tr>
      <td>367</td>
      <td>2</td>
      <td>CLIP is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval without OCR</td>
    </tr>
    <tr>
      <td>371</td>
      <td>2</td>
      <td>Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models</td>
    </tr>
    <tr>
      <td>375</td>
      <td>2</td>
      <td>Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training</td>
    </tr>
    <tr>
      <td>378</td>
      <td>2</td>
      <td>DH-Set: Improving Vision-Language Alignment with Diverse and Hybrid Set-Embeddings Learning</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 1</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>72</td>
      <td>1</td>
      <td>Event Ellipsometer: Event-based Mueller-Matrix Video Imaging</td>
    </tr>
    <tr>
      <td>123</td>
      <td>1</td>
      <td>Spatiotemporal Decoupling for Efficient Vision-Based Occupancy Forecasting</td>
    </tr>
    <tr>
      <td>129</td>
      <td>1</td>
      <td>GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control</td>
    </tr>
    <tr>
      <td>133</td>
      <td>1</td>
      <td>Embodied Scene Understanding for Vision Language Models via MetaVQA</td>
    </tr>
    <tr>
      <td>136</td>
      <td>1</td>
      <td>RADIOv2.5: Improved Baselines for Agglomerative Vision Foundation Models</td>
    </tr>
    <tr>
      <td>151</td>
      <td>1</td>
      <td>Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction</td>
    </tr>
    <tr>
      <td>153</td>
      <td>1</td>
      <td>Ego4o: Egocentric Human Motion Capture and Understanding from Multi-Modal Input</td>
    </tr>
    <tr>
      <td>160</td>
      <td>1</td>
      <td>AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward</td>
    </tr>
    <tr>
      <td>228</td>
      <td>1</td>
      <td>StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements</td>
    </tr>
    <tr>
      <td>233</td>
      <td>1</td>
      <td>MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning</td>
    </tr>
    <tr>
      <td>235</td>
      <td>1</td>
      <td>Goku: Flow Based Video Generative Foundation Models</td>
    </tr>
    <tr>
      <td>261</td>
      <td>1</td>
      <td>Multi-Group Proportional Representations for Text-to-Image Models</td>
    </tr>
    <tr>
      <td>282</td>
      <td>1</td>
      <td>Language-Guided Audio-Visual Learning for Long-Term Sports Assessment</td>
    </tr>
    <tr>
      <td>283</td>
      <td>1</td>
      <td>Mimir: Improving Video Diffusion Models for Precise Text Understanding</td>
    </tr>
    <tr>
      <td>301</td>
      <td>1</td>
      <td>Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</td>
    </tr>
    <tr>
      <td>302</td>
      <td>1</td>
      <td>Anchor-Aware Similarity Cohesion in Target Frames Enables Predicting Temporal Moment Boundaries in 2D</td>
    </tr>
    <tr>
      <td>307</td>
      <td>1</td>
      <td>EntitySAM: Segment Everything in Video</td>
    </tr>
    <tr>
      <td>308</td>
      <td>1</td>
      <td>SLADE: Shielding against Dual Exploits in Large Vision-Language Models</td>
    </tr>
    <tr>
      <td>310</td>
      <td>1</td>
      <td>Just Dance with pi! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection</td>
    </tr>
    <tr>
      <td>341</td>
      <td>1</td>
      <td>ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark</td>
    </tr>
    <tr>
      <td>348</td>
      <td>1</td>
      <td>Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Mutimodal Models</td>
    </tr>
    <tr>
      <td>349</td>
      <td>1</td>
      <td>RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings</td>
    </tr>
    <tr>
      <td>356</td>
      <td>1</td>
      <td>Separation of Powers: On Segregating Knowledge from Observation in LLM-enabled Knowledge-based Visual Question Answering</td>
    </tr>
    <tr>
      <td>368</td>
      <td>1</td>
      <td>FLAIR: VLM with Fine-grained Language-informed Image Representations</td>
    </tr>
    <tr>
      <td>369</td>
      <td>1</td>
      <td>Retaining Knowledge and Enhancing Long-Text Representations in CLIP through Dual-Teacher Distillation</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="pane6" role="tabpanel" aria-labelledby="tab6"><h6 class="mt-3">Score &gt; 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>263</td>
      <td>4</td>
      <td>Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval</td>
    </tr>
    <tr>
      <td>266</td>
      <td>3</td>
      <td>VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection</td>
    </tr>
    <tr>
      <td>279</td>
      <td>4</td>
      <td>VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models</td>
    </tr>
    <tr>
      <td>290</td>
      <td>3</td>
      <td>The Devil is in Temporal Token: High Quality Video Reasoning Segmentation</td>
    </tr>
    <tr>
      <td>339</td>
      <td>3</td>
      <td>Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding</td>
    </tr>
    <tr>
      <td>356</td>
      <td>3</td>
      <td>Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction</td>
    </tr>
    <tr>
      <td>357</td>
      <td>4</td>
      <td>Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 2</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>139</td>
      <td>2</td>
      <td>Reasoning Mamba: Hypergraph-Guided Region Relation Calculating for Weakly Supervised Affordance Grounding</td>
    </tr>
    <tr>
      <td>141</td>
      <td>2</td>
      <td>Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation</td>
    </tr>
    <tr>
      <td>277</td>
      <td>2</td>
      <td>ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams</td>
    </tr>
    <tr>
      <td>280</td>
      <td>2</td>
      <td>Flexible Frame Selection for Efficient Video Reasoning</td>
    </tr>
    <tr>
      <td>286</td>
      <td>2</td>
      <td>EventGPT: Event Stream Understanding with Multimodal Large Language Models</td>
    </tr>
    <tr>
      <td>292</td>
      <td>2</td>
      <td>Anomize: Better Open Vocabulary Video Anomaly Detection</td>
    </tr>
    <tr>
      <td>329</td>
      <td>2</td>
      <td>LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models</td>
    </tr>
    <tr>
      <td>332</td>
      <td>2</td>
      <td>Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering</td>
    </tr>
    <tr>
      <td>335</td>
      <td>2</td>
      <td>UNIALIGN: Scaling Multimodal Alignment within One Unified Model</td>
    </tr>
    <tr>
      <td>344</td>
      <td>2</td>
      <td>Incorporating Dense Knowledge Alignment into Unified Multimodal Representation Models</td>
    </tr>
    <tr>
      <td>348</td>
      <td>2</td>
      <td>SmartCLIP: Modular Vision-language Alignment with Identification Guarantees</td>
    </tr>
  </tbody>
</table></div><h6 class="mt-3">Score = 1</h6><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>score</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>114</td>
      <td>1</td>
      <td>Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection</td>
    </tr>
    <tr>
      <td>155</td>
      <td>1</td>
      <td>Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks</td>
    </tr>
    <tr>
      <td>215</td>
      <td>1</td>
      <td>VIRES: Video Instance Repainting via Sketch and Text Guided Generation</td>
    </tr>
    <tr>
      <td>236</td>
      <td>1</td>
      <td>STEPS: Sequential Probability Tensor Estimation for Text-to-Image Hard Prompt Search</td>
    </tr>
    <tr>
      <td>237</td>
      <td>1</td>
      <td>PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance Prediction</td>
    </tr>
    <tr>
      <td>254</td>
      <td>1</td>
      <td>SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model</td>
    </tr>
    <tr>
      <td>256</td>
      <td>1</td>
      <td>NSD-Imagery: A Benchmark Dataset for Extending fMRI Vision Decoding Methods to Mental Imagery</td>
    </tr>
    <tr>
      <td>261</td>
      <td>1</td>
      <td>Foley-Flow: Coordinated Video-to-Audio Generation with Masked Audio-Visual Alignment and Dynamic Conditional Flows</td>
    </tr>
    <tr>
      <td>262</td>
      <td>1</td>
      <td>Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment</td>
    </tr>
    <tr>
      <td>265</td>
      <td>1</td>
      <td>Sound Bridge: Associating Egocentric and Exocentric Videos via Audio Cues</td>
    </tr>
    <tr>
      <td>267</td>
      <td>1</td>
      <td>BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation</td>
    </tr>
    <tr>
      <td>268</td>
      <td>1</td>
      <td>SEAL: Semantic Attention Learning for Long Video Representation</td>
    </tr>
    <tr>
      <td>270</td>
      <td>1</td>
      <td>InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</td>
    </tr>
    <tr>
      <td>282</td>
      <td>1</td>
      <td>BIMBA: Selective-Scan Compression for Long-Range Video Question Answering</td>
    </tr>
    <tr>
      <td>285</td>
      <td>1</td>
      <td>Efficient Transfer Learning for Video-language Foundation Models</td>
    </tr>
    <tr>
      <td>291</td>
      <td>1</td>
      <td>M^3-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation</td>
    </tr>
    <tr>
      <td>293</td>
      <td>1</td>
      <td>UniSTD: Towards Unified Spatio-Temporal Learning across Diverse Disciplines</td>
    </tr>
    <tr>
      <td>294</td>
      <td>1</td>
      <td>Temporal Action Detection Model Compression by Progressive Block Drop</td>
    </tr>
    <tr>
      <td>296</td>
      <td>1</td>
      <td>Revealing Key Details to See Differences: A Novel Prototypical Perspective for Skeleton-based Action Recognition</td>
    </tr>
    <tr>
      <td>302</td>
      <td>1</td>
      <td>MEET: Towards Memory-Efficient Temporal Sparse Deep Neural Networks</td>
    </tr>
    <tr>
      <td>311</td>
      <td>1</td>
      <td>SCSegamba: Lightweight Structure-Aware Vision Mamba for Crack Segmentation in Structures</td>
    </tr>
    <tr>
      <td>323</td>
      <td>1</td>
      <td>VLMs-Guided Representation Distillation for Efficient Vision-Based Reinforcement Learning</td>
    </tr>
    <tr>
      <td>326</td>
      <td>1</td>
      <td>CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models</td>
    </tr>
    <tr>
      <td>340</td>
      <td>1</td>
      <td>Generative Zero-Shot Composed Image Retrieval</td>
    </tr>
    <tr>
      <td>341</td>
      <td>1</td>
      <td>IDEA: Inverted Text with Cooperative Deformable Aggregation for Multi-modal Object Re-Identification</td>
    </tr>
    <tr>
      <td>343</td>
      <td>1</td>
      <td>Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</td>
    </tr>
    <tr>
      <td>346</td>
      <td>1</td>
      <td>Non-Natural Image Understanding with Advancing Frequency-based Vision Encoders</td>
    </tr>
    <tr>
      <td>351</td>
      <td>1</td>
      <td>HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models</td>
    </tr>
    <tr>
      <td>355</td>
      <td>1</td>
      <td>Towards Understanding How Knowledge Evolves in Large Vision-Language Models</td>
    </tr>
    <tr>
      <td>358</td>
      <td>1</td>
      <td>HalLoc: Token-level Localization of Hallucinations for Vision Language Models</td>
    </tr>
    <tr>
      <td>359</td>
      <td>1</td>
      <td>Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding</td>
    </tr>
    <tr>
      <td>360</td>
      <td>1</td>
      <td>Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention</td>
    </tr>
  </tbody>
</table></div></div>
</div></div>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
</body></html>
