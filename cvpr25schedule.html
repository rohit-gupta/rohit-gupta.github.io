

<!DOCTYPE html><html><head>

<meta charset="utf-8">

<meta name="viewport" content="width=device-width, initial-scale=1">

<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">

<title>Curated V&L / Video Papers Schedule</title>

<style>

.table-responsive{margin-top:1rem;}

</style></head><body>

<div class="container my-3">

<h3 class="text-center">CVPR Poster Schedule — Vision‑Language & Video‑Centric Papers</h3>

<ul class="nav nav-tabs" id="myTab" role="tablist">



<li class="nav-item" role="presentation">

  <button class="nav-link active" id="s1-tab" data-bs-toggle="tab" data-bs-target="#s1" type="button" role="tab" aria-controls="s1" aria-selected="true">Session 1</button>

</li>



<li class="nav-item" role="presentation">

  <button class="nav-link " id="s2-tab" data-bs-toggle="tab" data-bs-target="#s2" type="button" role="tab" aria-controls="s2" aria-selected="false">Session 2</button>

</li>



<li class="nav-item" role="presentation">

  <button class="nav-link " id="s3-tab" data-bs-toggle="tab" data-bs-target="#s3" type="button" role="tab" aria-controls="s3" aria-selected="false">Session 3</button>

</li>



<li class="nav-item" role="presentation">

  <button class="nav-link " id="s4-tab" data-bs-toggle="tab" data-bs-target="#s4" type="button" role="tab" aria-controls="s4" aria-selected="false">Session 4</button>

</li>



<li class="nav-item" role="presentation">

  <button class="nav-link " id="s5-tab" data-bs-toggle="tab" data-bs-target="#s5" type="button" role="tab" aria-controls="s5" aria-selected="false">Session 5</button>

</li>



<li class="nav-item" role="presentation">

  <button class="nav-link " id="s6-tab" data-bs-toggle="tab" data-bs-target="#s6" type="button" role="tab" aria-controls="s6" aria-selected="false">Session 6</button>

</li>

</ul><div class="tab-content" id="myTabContent"><div class="tab-pane fade show active" id="s1" role="tabpanel" aria-labelledby="s1-tab"><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>143</td>
      <td>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models</td>
    </tr>
    <tr>
      <td>177</td>
      <td>ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning</td>
    </tr>
    <tr>
      <td>205</td>
      <td>Distilling Spatially-Heterogeneous Distortion Perception for Blind Image Quality Assessment</td>
    </tr>
    <tr>
      <td>296</td>
      <td>Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning</td>
    </tr>
    <tr>
      <td>301</td>
      <td>BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding</td>
    </tr>
    <tr>
      <td>309</td>
      <td>Segment Any Motion in Videos</td>
    </tr>
    <tr>
      <td>314</td>
      <td>Bridging Gait Recognition and Large Language Models Sequence Modeling</td>
    </tr>
    <tr>
      <td>315</td>
      <td>DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos</td>
    </tr>
    <tr>
      <td>335</td>
      <td>SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories</td>
    </tr>
    <tr>
      <td>344</td>
      <td>Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal Language Model</td>
    </tr>
    <tr>
      <td>346</td>
      <td>VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation</td>
    </tr>
    <tr>
      <td>347</td>
      <td>GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks</td>
    </tr>
    <tr>
      <td>349</td>
      <td>Perception Tokens Enhance Visual Reasoning in Multimodal Language Models</td>
    </tr>
    <tr>
      <td>352</td>
      <td>Words or Vision: Do Vision-Language Models Have Blind Faith in Text?</td>
    </tr>
    <tr>
      <td>356</td>
      <td>FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering</td>
    </tr>
    <tr>
      <td>357</td>
      <td>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks</td>
    </tr>
    <tr>
      <td>358</td>
      <td>Task-aware Cross-modal Feature Refinement Transformer with Large Language Models for Visual Grounding</td>
    </tr>
    <tr>
      <td>370</td>
      <td>Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</td>
    </tr>
    <tr>
      <td>375</td>
      <td>VladVA: Discriminative Fine-tuning of LVLMs</td>
    </tr>
    <tr>
      <td>377</td>
      <td>NVILA: Efficient Frontier Visual Language Models</td>
    </tr>
    <tr>
      <td>379</td>
      <td>BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices</td>
    </tr>
    <tr>
      <td>380</td>
      <td>Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices</td>
    </tr>
    <tr>
      <td>383</td>
      <td>Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Large Model Enhancement</td>
    </tr>
    <tr>
      <td>392</td>
      <td>Adaptive Parameter Selection for Tuning Vision-Language Models</td>
    </tr>
    <tr>
      <td>423</td>
      <td>DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for Robust Few-Shot Segmentation</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="s2" role="tabpanel" aria-labelledby="s2-tab"><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>16</td>
      <td>ControlFace: Harnessing Facial Parametric Control for Face Rigging</td>
    </tr>
    <tr>
      <td>143</td>
      <td>MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning in Assistive Driving Perception</td>
    </tr>
    <tr>
      <td>177</td>
      <td>Seurat: From Moving Points to Depth</td>
    </tr>
    <tr>
      <td>181</td>
      <td>Autoregressive Sequential Pretraining for Visual Tracking</td>
    </tr>
    <tr>
      <td>287</td>
      <td>Contextual AD Narration with Interleaved Multimodal Sequence</td>
    </tr>
    <tr>
      <td>296</td>
      <td>MMVU: Measuring Expert-Level Multi-Discipline Video Understanding</td>
    </tr>
    <tr>
      <td>309</td>
      <td>PHGC: Procedural Heterogeneous Graph Completion for Natural Language Task Verification in Egocentric Videos</td>
    </tr>
    <tr>
      <td>315</td>
      <td>LiVOS: Light Video Object Segmentation with Gated Linear Matching</td>
    </tr>
    <tr>
      <td>318</td>
      <td>Context-Enhanced Memory-Refined Transformer for Online Action Detection</td>
    </tr>
    <tr>
      <td>319</td>
      <td>Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer</td>
    </tr>
    <tr>
      <td>334</td>
      <td>Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models</td>
    </tr>
    <tr>
      <td>335</td>
      <td>Advancing Manga Analysis: Comprehensive Segmentation Annotations for the Manga109 Dataset</td>
    </tr>
    <tr>
      <td>344</td>
      <td>Textured Gaussians for Enhanced 3D Scene Appearance Modeling</td>
    </tr>
    <tr>
      <td>347</td>
      <td>Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding</td>
    </tr>
    <tr>
      <td>350</td>
      <td>ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction</td>
    </tr>
    <tr>
      <td>357</td>
      <td>Taxonomy-Aware Evaluation of Vision-Language Models</td>
    </tr>
    <tr>
      <td>358</td>
      <td>Fine-Grained Erasure in Text-to-Image Diffusion-based Foundation Models</td>
    </tr>
    <tr>
      <td>364</td>
      <td>COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts</td>
    </tr>
    <tr>
      <td>375</td>
      <td>CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation</td>
    </tr>
    <tr>
      <td>377</td>
      <td>SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting</td>
    </tr>
    <tr>
      <td>385</td>
      <td>AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization</td>
    </tr>
    <tr>
      <td>386</td>
      <td>Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization</td>
    </tr>
    <tr>
      <td>423</td>
      <td>SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation</td>
    </tr>
    <tr>
      <td>427</td>
      <td>Beyond Image Classification: A Video Benchmark and Dual-Branch Hybrid Discrimination Framework for Compositional Zero-Shot Learning</td>
    </tr>
    <tr>
      <td>478</td>
      <td>Boost the Inference with Co-training: A Depth-guided Mutual Learning Framework for Semi-supervised Medical Polyp Segmentation</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="s3" role="tabpanel" aria-labelledby="s3-tab"><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>90</td>
      <td>Bridging Viewpoint Gaps: Geometric Reasoning Boosts Semantic Correspondence</td>
    </tr>
    <tr>
      <td>143</td>
      <td>IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments</td>
    </tr>
    <tr>
      <td>168</td>
      <td>Adapting Pre-trained 3D Models for Point Cloud Video Understanding via Cross-frame Spatio-temporal Perception</td>
    </tr>
    <tr>
      <td>177</td>
      <td>Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think</td>
    </tr>
    <tr>
      <td>268</td>
      <td>CO-SPY: Combining Semantic and Pixel Features to Detect Synthetic Images by AI</td>
    </tr>
    <tr>
      <td>288</td>
      <td>Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations</td>
    </tr>
    <tr>
      <td>296</td>
      <td>MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations</td>
    </tr>
    <tr>
      <td>307</td>
      <td>Boosting Point-Supervised Temporal Action Localization through Integrating Query Reformation and Optimal Transport</td>
    </tr>
    <tr>
      <td>309</td>
      <td>Mono3DVLT: Monocular-Video-Based 3D Visual Language Tracking</td>
    </tr>
    <tr>
      <td>335</td>
      <td>Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning</td>
    </tr>
    <tr>
      <td>344</td>
      <td>Filter Images First, Generate Instructions Later: Pre-Instruction Data Selection for Visual Instruction Tuning</td>
    </tr>
    <tr>
      <td>346</td>
      <td>Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought</td>
    </tr>
    <tr>
      <td>347</td>
      <td>Model Diagnosis and Correction via Linguistic and Implicit Attribute Editing</td>
    </tr>
    <tr>
      <td>349</td>
      <td>EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues</td>
    </tr>
    <tr>
      <td>352</td>
      <td>DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels</td>
    </tr>
    <tr>
      <td>356</td>
      <td>BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs</td>
    </tr>
    <tr>
      <td>358</td>
      <td>Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content</td>
    </tr>
    <tr>
      <td>361</td>
      <td>Active Data Curation Effectively Distills Large-Scale Multimodal Models</td>
    </tr>
    <tr>
      <td>365</td>
      <td>A Simple yet Effective Layout Token in Large Language Models for Document Understanding</td>
    </tr>
    <tr>
      <td>369</td>
      <td>Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation</td>
    </tr>
    <tr>
      <td>380</td>
      <td>Continual SFT Matches Multimodal RLHF with Negative Supervision</td>
    </tr>
    <tr>
      <td>381</td>
      <td>ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models</td>
    </tr>
    <tr>
      <td>386</td>
      <td>Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks</td>
    </tr>
    <tr>
      <td>387</td>
      <td>COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training</td>
    </tr>
    <tr>
      <td>452</td>
      <td>A Unified Framework for Heterogeneous Semi-supervised Learning</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="s4" role="tabpanel" aria-labelledby="s4-tab"><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>103</td>
      <td>All-Day Multi-Camera Multi-Target Tracking</td>
    </tr>
    <tr>
      <td>143</td>
      <td>MobileH2R: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data</td>
    </tr>
    <tr>
      <td>177</td>
      <td>Buffer Anytime: Zero-Shot Video Depth and Normal from Image Priors</td>
    </tr>
    <tr>
      <td>217</td>
      <td>Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration</td>
    </tr>
    <tr>
      <td>296</td>
      <td>Apollo: An Exploration of Video Understanding in Large Multimodal Models</td>
    </tr>
    <tr>
      <td>301</td>
      <td>Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs</td>
    </tr>
    <tr>
      <td>318</td>
      <td>Action Detail Matters: Refining Video Recognition with Local Action Queries</td>
    </tr>
    <tr>
      <td>319</td>
      <td>CountLLM: Towards Generalizable Repetitive Action Counting via Large Language Model</td>
    </tr>
    <tr>
      <td>324</td>
      <td>CryptoFace: End-to-End Encrypted Face Recognition</td>
    </tr>
    <tr>
      <td>335</td>
      <td>PolarNeXt: Rethink Instance Segmentation with Polar Representation</td>
    </tr>
    <tr>
      <td>344</td>
      <td>SKE-Layout: Spatial Knowledge Enhanced Layout Generation with LLMs</td>
    </tr>
    <tr>
      <td>346</td>
      <td>Empowering Large Language Models with 3D Situation Awareness</td>
    </tr>
    <tr>
      <td>349</td>
      <td>GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs</td>
    </tr>
    <tr>
      <td>352</td>
      <td>ShowUI: One Vision-Language-Action Model for GUI Visual Agent</td>
    </tr>
    <tr>
      <td>357</td>
      <td>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</td>
    </tr>
    <tr>
      <td>358</td>
      <td>All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages</td>
    </tr>
    <tr>
      <td>362</td>
      <td>CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering</td>
    </tr>
    <tr>
      <td>363</td>
      <td>Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning</td>
    </tr>
    <tr>
      <td>370</td>
      <td>Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval</td>
    </tr>
    <tr>
      <td>374</td>
      <td>BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature</td>
    </tr>
    <tr>
      <td>375</td>
      <td>Visual Lexicon: Rich Image Features in Language Space</td>
    </tr>
    <tr>
      <td>377</td>
      <td>AdaDARE-gamma: Balancing Stability and Plasticity in Multi-modal LLMs through Efficient Adaptation</td>
    </tr>
    <tr>
      <td>390</td>
      <td>Anyattack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models</td>
    </tr>
    <tr>
      <td>423</td>
      <td>Soft Self-labeling and Potts Relaxations for Weakly-supervised Segmentation</td>
    </tr>
    <tr>
      <td>441</td>
      <td>Curriculum Coarse-to-Fine Selection for High-IPC Dataset Distillation</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="s5" role="tabpanel" aria-labelledby="s5-tab"><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>90</td>
      <td>BLADE: Single-view Body Mesh Estimation through Accurate Depth Estimation</td>
    </tr>
    <tr>
      <td>115</td>
      <td>Distilling Monocular Foundation Model for Fine-grained Depth Completion</td>
    </tr>
    <tr>
      <td>168</td>
      <td>Align3R: Aligned Monocular Depth Estimation for Dynamic Videos</td>
    </tr>
    <tr>
      <td>287</td>
      <td>DejaVid: Encoder-Agnostic Learned Temporal Matching for Video Classification</td>
    </tr>
    <tr>
      <td>293</td>
      <td>Cross-modal Causal Relation Alignment for Video Question Grounding</td>
    </tr>
    <tr>
      <td>296</td>
      <td>Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models</td>
    </tr>
    <tr>
      <td>301</td>
      <td>Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</td>
    </tr>
    <tr>
      <td>302</td>
      <td>Anchor-Aware Similarity Cohesion in Target Frames Enables Predicting Temporal Moment Boundaries in 2D</td>
    </tr>
    <tr>
      <td>304</td>
      <td>Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events</td>
    </tr>
    <tr>
      <td>312</td>
      <td>DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer</td>
    </tr>
    <tr>
      <td>314</td>
      <td>Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?</td>
    </tr>
    <tr>
      <td>315</td>
      <td>VSNet: Focusing on the Linguistic Characteristics of Sign Language</td>
    </tr>
    <tr>
      <td>344</td>
      <td>The Scene Language: Representing Scenes with Programs, Words, and Embeddings</td>
    </tr>
    <tr>
      <td>349</td>
      <td>RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings</td>
    </tr>
    <tr>
      <td>356</td>
      <td>Separation of Powers: On Segregating Knowledge from Observation in LLM-enabled Knowledge-based Visual Question Answering</td>
    </tr>
    <tr>
      <td>357</td>
      <td>FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity</td>
    </tr>
    <tr>
      <td>358</td>
      <td>AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment</td>
    </tr>
    <tr>
      <td>370</td>
      <td>DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment</td>
    </tr>
    <tr>
      <td>375</td>
      <td>Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training</td>
    </tr>
    <tr>
      <td>376</td>
      <td>ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models</td>
    </tr>
    <tr>
      <td>377</td>
      <td>It’s a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data</td>
    </tr>
    <tr>
      <td>378</td>
      <td>DH-Set: Improving Vision-Language Alignment with Diverse and Hybrid Set-Embeddings Learning</td>
    </tr>
    <tr>
      <td>420</td>
      <td>Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement</td>
    </tr>
    <tr>
      <td>423</td>
      <td>Detect Any Mirrors: Boosting Learning Reliability on Large-Scale Unlabeled Data with an Iterative Data Engine</td>
    </tr>
    <tr>
      <td>463</td>
      <td>Adaptive Unimodal Regulation for Balanced Multimodal Information Acquisition</td>
    </tr>
  </tbody>
</table></div></div><div class="tab-pane fade " id="s6" role="tabpanel" aria-labelledby="s6-tab"><div class="table-responsive"><table border="1" class="dataframe table table-striped table-sm">
  <thead>
    <tr style="text-align: right;">
      <th>poster_id</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>76</td>
      <td>Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion</td>
    </tr>
    <tr>
      <td>177</td>
      <td>ProReflow: Progressive Reflow with Decomposed Velocity</td>
    </tr>
    <tr>
      <td>267</td>
      <td>BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation</td>
    </tr>
    <tr>
      <td>282</td>
      <td>BIMBA: Selective-Scan Compression for Long-Range Video Question Answering</td>
    </tr>
    <tr>
      <td>292</td>
      <td>Anomize: Better Open Vocabulary Video Anomaly Detection</td>
    </tr>
    <tr>
      <td>296</td>
      <td>Revealing Key Details to See Differences: A Novel Prototypical Perspective for Skeleton-based Action Recognition</td>
    </tr>
    <tr>
      <td>301</td>
      <td>UMFN: Unified Multi-Domain Face Normalization for Joint Cross-domain Prototype Learning and Heterogeneous Face Recognition</td>
    </tr>
    <tr>
      <td>309</td>
      <td>Segment Anything, Even Occluded</td>
    </tr>
    <tr>
      <td>315</td>
      <td>Navigating the Unseen: Zero-shot Scene Graph Generation via Capsule-Based Equivariant Features</td>
    </tr>
    <tr>
      <td>320</td>
      <td>3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination</td>
    </tr>
    <tr>
      <td>326</td>
      <td>CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models</td>
    </tr>
    <tr>
      <td>328</td>
      <td>CocoER: Aligning Multi-Level Feature by Competition and Coordination for Emotion Recognition</td>
    </tr>
    <tr>
      <td>332</td>
      <td>Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering</td>
    </tr>
    <tr>
      <td>335</td>
      <td>UNIALIGN: Scaling Multimodal Alignment within One Unified Model</td>
    </tr>
    <tr>
      <td>344</td>
      <td>Incorporating Dense Knowledge Alignment into Unified Multimodal Representation Models</td>
    </tr>
    <tr>
      <td>346</td>
      <td>Non-Natural Image Understanding with Advancing Frequency-based Vision Encoders</td>
    </tr>
    <tr>
      <td>347</td>
      <td>SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding</td>
    </tr>
    <tr>
      <td>349</td>
      <td>Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training</td>
    </tr>
    <tr>
      <td>356</td>
      <td>Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction</td>
    </tr>
    <tr>
      <td>357</td>
      <td>Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment</td>
    </tr>
    <tr>
      <td>358</td>
      <td>HalLoc: Token-level Localization of Hallucinations for Vision Language Models</td>
    </tr>
    <tr>
      <td>361</td>
      <td>BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models</td>
    </tr>
    <tr>
      <td>375</td>
      <td>VL2Lite: Task-Specific Knowledge Distillation from Large Vision-Language Models to Lightweight Networks</td>
    </tr>
    <tr>
      <td>377</td>
      <td>Probing the Mid-level Vision Capabilities of Self-Supervised Learning</td>
    </tr>
    <tr>
      <td>380</td>
      <td>Sample- and Parameter-Efficient Auto-Regressive Image Models</td>
    </tr>
  </tbody>
</table></div></div></div>

</div>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>

</body></html>

